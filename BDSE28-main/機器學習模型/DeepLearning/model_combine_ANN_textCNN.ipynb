{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "#import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('job7_df_all.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24096, 771)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.dropna().reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN 預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>...</th>\n",
       "      <th>c55</th>\n",
       "      <th>c56</th>\n",
       "      <th>c57</th>\n",
       "      <th>c58</th>\n",
       "      <th>c59</th>\n",
       "      <th>c60</th>\n",
       "      <th>c61</th>\n",
       "      <th>c62</th>\n",
       "      <th>c63</th>\n",
       "      <th>c64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>465</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>84</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "      <td>244</td>\n",
       "      <td>119</td>\n",
       "      <td>462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>395</td>\n",
       "      <td>206</td>\n",
       "      <td>187</td>\n",
       "      <td>418</td>\n",
       "      <td>248</td>\n",
       "      <td>226</td>\n",
       "      <td>5</td>\n",
       "      <td>439</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>353</td>\n",
       "      <td>88</td>\n",
       "      <td>476</td>\n",
       "      <td>36</td>\n",
       "      <td>248</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>465</td>\n",
       "      <td>409</td>\n",
       "      <td>110</td>\n",
       "      <td>476</td>\n",
       "      <td>374</td>\n",
       "      <td>248</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    c0   c1   c2   c3   c4   c5   c6   c7   c8   c9  ...  c55  c56  c57  c58  \\\n",
       "0  149  465   29    1   64   84  354    0    0    0  ...    0    0    0    0   \n",
       "1  133  244  119  462    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2   88   29  395  206  187  418  248  226    5  439  ...    0    0    0    0   \n",
       "3  149  353   88  476   36  248  110    5  106    0  ...    0    0    0    0   \n",
       "4  149  465  409  110  476  374  248  110    5    0  ...    0    0    0    0   \n",
       "\n",
       "   c59  c60  c61  c62  c63  c64  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cnn = df.iloc[:,705:-1].copy()\n",
    "df_cnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.pickle', 'rb') as handle:\n",
    "    # vocab = pickle.load(handle)\n",
    "    \n",
    "# num_words = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 613\n",
    "max_train_sentence_length = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_matrix15.pkl','rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN 預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3G', '3ds-Max', '3ds-Max-Design', '6-Sigma', 'A+', 'ABAQUS', 'ADA',\n",
       "       'ADC', 'ADO', 'ADSL',\n",
       "       ...\n",
       "       '英文打字150以上', '英文打字20~50', '英文打字20以下', '英文打字50~75', '英文打字75~100',\n",
       "       '金旭飯店管裡系統', '金蝶', '鉅茂', '鼎基-ERP', '鼎新'],\n",
       "      dtype='object', length=500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,194:-77].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['上班時段_label', '縣市_label', '職位類別_label'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,-77:-74].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies\n",
    "\n",
    "df_pst_ = pd.get_dummies(df['職位_'])\n",
    "df_pst_cat = pd.get_dummies(df['職位類別'])\n",
    "df_country = pd.get_dummies(df['縣市'])\n",
    "df_area = pd.get_dummies(df['地區'])\n",
    "df_time = pd.get_dummies(df['上班時段'])\n",
    "\n",
    "df_edu_ = pd.get_dummies(df['學歷要求_'])\n",
    "df_res_ = pd.get_dummies(df['管理責任_'])\n",
    "df_dem_ = pd.get_dummies(df['需求人數_'])\n",
    "df_work_ = pd.get_dummies(df['工作經歷'])\n",
    "\n",
    "#擅長工具\n",
    "df_tools = (df.iloc[:,193:-77]).copy()\n",
    "df_newlabel = (df.iloc[:,-77:-74]).copy()\n",
    "df = df.astype({'供需人數':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['JavaScript', 'Linux', 'C#', 'MS-SQL', 'Java', 'Python', 'C++', 'HTML',\n",
       "       'C', 'MySQL', 'ASP.NET', 'CSS', 'jQuery', 'Git', 'Excel', 'Word', 'PHP',\n",
       "       'PowerPoint', 'Oracle', 'TCP/IP', 'VueJS', 'Android', 'AWS', 'Node.js',\n",
       "       'Vmware', 'AJAX', 'Outlook', 'Windows-Server-2012', 'Spring',\n",
       "       'Windows-10', 'Firewall', 'ReactJS', 'iOS', 'Github', 'PostgreSQL',\n",
       "       'Visual-Studio', 'PL/SQL', 'DNS', 'Visual-Studio-.net',\n",
       "       'Windows-Server-2019', 'Shell', 'SWIFT', 'Go', 'Cisco', 'Visual-C#',\n",
       "       '鼎新', 'HTTP', 'JSP', 'Visual-Basic-.net', 'Angular'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 篩選擅長工具\n",
    "top_tools = ['JavaScript', 'Linux', 'C#', 'MS-SQL', 'Java', 'Python', 'C++', 'HTML', 'C', 'MySQL', 'ASP.NET', 'CSS', 'jQuery', 'Git', 'Excel', 'Word', 'PHP', 'PowerPoint', 'Oracle', 'TCP/IP', 'VueJS', 'Android', 'AWS', 'Node.js', 'Vmware', 'AJAX', 'Outlook', 'Windows-Server-2012', 'Spring', 'Windows-10', 'Firewall', 'ReactJS', 'iOS', 'Github', 'PostgreSQL', 'Visual-Studio', 'PL/SQL', 'DNS', 'Visual-Studio-.net', 'Windows-Server-2019', 'Shell', 'SWIFT', 'Go', 'Cisco', 'Visual-C#', '鼎新', 'HTTP', 'JSP', 'Visual-Basic-.net', 'Angular', 'Windows-Server-2008', 'Windows-7', 'MCU', 'Objective-C', 'Kotlin', 'Unity3D', 'XML', 'VPN', 'LAN', 'SAP', 'AngularJS', 'UNIX', 'DHCP', 'Matlab', 'C++.Net', 'Tomcat', 'IIS', 'Visual-C++', 'ARM', 'OOP', 'Power-BI', 'Visual-Basic', 'Jenkins', 'VLAN', 'ETL', 'Windows-XP', 'Ethernet', 'J2EE', 'WinForm', 'UML', 'Mac-OS', 'Windows-8', 'RDBMS', 'FTP', 'Django', 'Adobe-Photoshop', 'PLC', 'Sass', 'Socket', 'ReactNative', 'Tableau', 'Security', 'Microsoft-Exchange', 'R', 'ASP', 'Illustrator', '中文打字20~50', 'Struts', 'AutoCAD', 'Routers', 'Firmware', 'WAN', '中文打字50~75', 'Project', 'hadoop', 'LabVIEW', 'Google-Analytics', 'VBA', 'MES', 'DHTML', 'JDBC', 'Visio', 'Assembly', 'XML-Web-services', '英文打字20~50', 'Windows-2000', 'Windows-2003', 'Access', 'Systems-Analysis', 'Database-Management', 'ANSI-SQL', 'Delphi', 'Oracle-ERP', 'FPGA', 'ssh', 'UDP', 'Ruby', 'Verilog', 'Perl', 'VBScript', 'Redux', 'Flutter', 'Internet-Explorer', 'Load-Balancing', 'Windows-NT', 'Web-Master/Developer', 'COBOL', 'DB2', 'LDAP', 'MFC', 'Adobe-Acrobat', 'Database-Administrator', '英文打字50~75', 'SAN/NAS', 'Servlets', 'Juniper', 'Citrix', 'AS/400', 'Unreal-Engine', 'Adobe-XD', 'SAN', 'DSP', 'Microsoft-SharePoint', 'ODBC', 'OOAD', 'Informatica', 'XHTML', 'SAS', 'Sketch', 'Systems-Administration', 'VM', 'IPS', 'Firebase', 'Hubs/-Routers', 'Rails', 'Data-Modeling', 'TcpDump', 'Sniffer', 'WLAN', 'Bluetooth', 'Informix', 'OrCAD', 'Apache-SOAP', 'IDS', 'RTL', 'Windows-98', 'Axure-RP', 'ADO', 'SolidWorks', 'AIX', 'Apple', 'J2SE', 'Scala', 'Hive', 'JSF', 'GIS', 'RPG', 'WebSphere', 'TCL', 'EDA', 'PowerBuilder', 'FreeBSD', 'ActiveX', 'Lotus-Notes', 'iptables', 'Google-Tag-Manager', 'OSPF', 'Win32', 'e-commerce', 'DataStage', 'Domino', 'MPLS', 'OLAP', 'SNMP', 'VHDL', 'CIM', 'Mac/Macintosh', 'SPSS', 'Oracle-Forms', 'Systems-Analyst', 'Teradata', 'BGP', 'Data-Architect', 'Dreamweaver', 'WebLogic', 'Figma', '正航', 'Checkpoint', 'MAYA', 'OS-X', 'SDLC', 'Ghost', '3ds-Max', 'Google-Data-Studio', 'Solaris', 'Zeplin', 'Drivers', 'DOS', 'SAPDB', 'SOAP', 'VoIP', 'Windows-95', '文中系統', 'D3.js', 'Junit', 'Xmind', 'Robot', 'WAP', 'Windows-Vista', 'PADS', 'FrontPage', 'MQSeries', 'VSAM', '英文打字75~100', 'Cognos', 'Dart', 'Flex', 'USB技術', 'WebAssembly', 'ArcGis', 'Data-Marts', 'Google-Trend', 'PPPoE', 'Fortify', 'MRP', 'Sun-Solaris', 'Cadence-Allegro', 'LotusScript', 'Protel', 'Developer/-Designer-2000', 'STL', 'HP-UX', 'Squid', 'After-Effects', 'AutoCad-2D', 'Dbase', '中文打字75~100', 'Circuit-Design', 'Mantis', 'ASIC', 'EJB', 'Sybase', 'VMS', 'Version-Control', 'ADC', 'Adobe-InDesign', 'Blender', 'CGI', 'Toad', 'XSL', '天心資訊', 'DirectX', 'Flash', 'Intrusion', 'OS/400', 'Premiere', 'Sketch-up', 'AutoCad-3D', 'LibreOffice-Writer', '德安飯店餐飲管理系統', 'Bugzilla', 'COM/DCOM', 'PBX', 'RF', 'Test-Scripts', 'Visual-Foxpro', 'WIN-CE', '中文打字20以下', 'Jasmine', 'Publisher', '德安資訊ERP', '鼎基-ERP', 'EDI', 'Fox-Pro', 'InVision', 'Navision', 'Pro/E', '英文打字20以下', 'AdvanceLink', 'CPLD', 'Data-Guard', 'Fireworks', 'SPC', 'Avaya', 'C++test', 'CAM', 'Draw', 'IMS', 'J2ME', 'Microsoft-Dynamics-AX', 'Network-Cards', 'SUN-OS', 'Screaming-Frog-SEO-Spider', 'Silverlight', 'VERITAS', 'ADSL', 'Base', 'Bridges', 'CorelDraw', 'Shtml', 'CC-Mail', 'Electronic-Payment', 'Games', 'Graphics', 'Hubs', 'JCL', 'Pro*C', 'Progress', '英文打字100~125', '鉅茂', 'Alexa', 'Calc', 'Google-Display-Network', 'Impress', 'MicroStrategy', 'OS/390', 'OneNote', 'RIP', 'ActionScript', 'BS7799', 'Clipper', 'GPS全球定位系統', 'IE工業工程', 'LoadRunner', 'PhotoImpact', 'SQR', '金蝶', 'CA', 'Mainframe', 'Microsoft-Photo-Editor', 'Quark-Express', 'Adabas', 'CASE', 'Cinema-4D', 'GSM', 'JMS', 'PowerPCB', 'SMT', 'Wordperfect', '用友U8', '6-Sigma', 'CICS', 'CodeTest', 'FrameMaker', 'ISDN', 'Inventor', 'LanManager', 'P-CAD', 'Pagemaker', 'Planner', 'SYSBASE', 'SimilarWeb', 'WML', 'Windows-Mobile', 'XSLT', '中文打字100~125', 'A+', 'FoxBASE+', 'GPRS', 'ISAPI', 'ISO-9000', 'Motion-Builder', 'Netbios', 'QTP', 'RTSP', 'SuperGIS', 'TIBCO', 'TS16949', 'Visual-J#', 'WebMethods', '上華ERP系統', '浪潮', '金旭飯店管裡系統', 'CVS', 'Catia', 'DDK', 'ERwin', 'FORTRAN', 'FoxPro-2', 'Hyperion-(Brio)', 'ISO-14000', 'Ingres', 'Macromedia-Director', 'Oracle-Financials', 'RoHS', 'Sonet', 'Sublime', 'Baan', 'Banyan', 'CDMA', 'CORBA', 'DVB數位視頻廣播', 'EMC/EMI', 'MIDI', 'MMS', 'MapGIS', 'Mobile-phone', 'Multimedia-Builder', 'NDS/Novell-Directory-Services', 'OS/2', 'OmniGraffle', 'PC—lint', 'PL/1', 'SPICE', 'Scribus', 'Synopsys', 'Telecom', 'TestBed', 'V-Ray', 'WPS', 'X++', 'X.25', 'Zbrush', '中文打字125~150', '中文打字150以上', '3ds-Max-Design', 'ABAQUS', 'ATL', 'Adobe-Animate', 'BizTalk', 'ClearCase', 'ClearQuest', 'Cold-Fusion', 'DEC/VAX', 'DVR數位視頻錄像', 'FileNet', 'Focus', 'HP-Open-View', 'IATF16949', 'LanServer', 'Lantastic', 'Lease-Lines', 'Math', 'Microsoft-SmartPhone', 'NetWare', 'PABX', 'PCBA', 'PSTN', 'QAD－MFG/PRO', 'Revit', 'SMS', 'SNA', 'Silverstream', 'Softimage', 'USB-OTG', 'Visual-SourceSafe', 'VxWorks', '英文打字125~150', '英文打字150以上', 'ADA', 'ArcView', 'Authorware', 'CADAM', 'CoolDraw', 'ISO-45001', 'MVS', 'Mindnode', 'Mobile-Network', 'PDA/Handhelds', 'PTC-Creo-Elements/Direct', 'RMI', 'Rational-Robot', 'Rational-Test-RealTime', 'Rexx', 'Rhino', 'SOLIDWORKS-Electrical', 'TK', 'Tandem', 'Unigraphics', '3G']\n",
    "\n",
    "# choose top tools\n",
    "df_tools = df_tools.loc[:, top_tools[:50]].reset_index(drop=True)\n",
    "df_tools.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "col_list = ['職位類別_label', '縣市_label', '上班時段_label', '外商', '供需人數']\n",
    "df_ = df.loc[:, col_list].reset_index(drop=True)\n",
    "\n",
    "df_label = pd.concat([df_, df_pst_, df_area, df_edu_, df_res_, df_dem_, df_work_, df_tools, df_newlabel], axis=1)\n",
    "\n",
    "\n",
    "# dummies\n",
    "col_list2 = ['外商', '供需人數']\n",
    "df2_ = df.loc[:, col_list2].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_dummies = pd.concat([df2_, df_pst_cat, df_pst_, df_country, df_time,\n",
    "                        df_edu_, df_res_, df_dem_, df_work_, df_tools, df_newlabel], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 兩種版本\n",
    "# df_ann = df_label.copy()\n",
    "df_ann = df_dummies.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training/ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df[['salary_avg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_cnn, df_ann, df[['salary_avg']]], axis=1)\n",
    "df_concat = shuffle(df_concat, random_state = 28374)\n",
    "salary = df_concat['salary_avg'].tolist()\n",
    "df_concat = df_concat.drop('salary_avg', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df_concat,\\\n",
    "                                                      salary,\\\n",
    "                                                      test_size = 0.1,  \n",
    "                                                      random_state = 887762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (21686, 164)\n",
      "x_valid shape (2410, 164)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape', X_train.shape)\n",
    "print('x_valid shape', X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cnn shape (21686, 65)\n",
      "X_valid_cnn shape (2410, 65)\n",
      "X_train_ann shape (21686, 99)\n",
      "X_valid_ann shape (2410, 99)\n",
      "y_train shape (21686,)\n",
      "y_valid shape (2410,)\n"
     ]
    }
   ],
   "source": [
    "# cnn\n",
    "X_train_cnn = X_train.iloc[:, :65]\n",
    "X_valid_cnn = X_valid.iloc[:, :65]\n",
    "# X_train_cnn = X_train_cnn.values.tolist()\n",
    "# X_valid_cnn = X_valid_cnn.values.tolist()\n",
    "# X_train_cnn = np.asarray(X_train_cnn)\n",
    "# X_valid_cnn = np.asarray(X_valid_cnn)\n",
    "print('X_train_cnn shape', X_train_cnn.shape)\n",
    "print('X_valid_cnn shape', X_valid_cnn.shape)\n",
    "\n",
    "# ann\n",
    "X_train_ann = X_train.iloc[:, 65:]\n",
    "X_valid_ann = X_valid.iloc[:, 65:]\n",
    "# X_train_ann = np.asarray(X_train_ann)\n",
    "# X_valid_ann = np.asarray(X_valid_ann)\n",
    "print('X_train_ann shape', X_train_ann.shape)\n",
    "print('X_valid_ann shape', X_valid_ann.shape)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "# valid_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_valid = np.asarray(y_valid)\n",
    "print('y_train shape', y_train.shape)\n",
    "print('y_valid shape', y_valid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(X_train_cnn, X_valid_cnn, X_train_ann, X_valid_ann, y_train, y_valid, embedding_matrix):\n",
    "\n",
    "    # 構建TextCNN模型\n",
    "    cnn_input = tf.keras.layers.Input(shape=(max_train_sentence_length,), dtype='float64')\n",
    "\n",
    "    # embedding\n",
    "    embedder = tf.keras.layers.Embedding(num_words + 1, 15, input_length=max_train_sentence_length, \n",
    "                                            weights=[embedding_matrix], trainable=False)\n",
    "    embed = embedder(cnn_input)\n",
    "\n",
    "    # cnn1\n",
    "    cnn1 = tf.keras.layers.Conv1D(256, 3, padding='same', strides=1, \n",
    "                                    activation='relu',\\\n",
    "                                    kernel_initializer='he_normal',\\\n",
    "                                    kernel_regularizer = regularizers.l2(0.0004))(embed)\n",
    "\n",
    "    cnn1 = tf.keras.layers.Conv1D(256, 3, padding='same', strides=1, \n",
    "                                activation='relu',\\\n",
    "                                kernel_initializer='he_normal',\\\n",
    "                                kernel_regularizer = regularizers.l2(0.0004))(cnn1)\n",
    "    #max pooling                         \n",
    "    # cnn1 = tf.keras.layers.MaxPooling1D(pool_size=3, padding='valid')(cnn1)\n",
    "\n",
    "    cnn1 = tf.keras.layers.Conv1D(128, 3, padding='same', strides=1, \n",
    "                                    activation='relu',\\\n",
    "                                    kernel_initializer='he_normal',\\\n",
    "                                    kernel_regularizer = regularizers.l2(0.0004))(cnn1)\n",
    "\n",
    "    cnn1 = tf.keras.layers.Conv1D(128, 3, padding='same', strides=1, \n",
    "                                activation='relu',\\\n",
    "                                kernel_initializer='he_normal',\\\n",
    "                                kernel_regularizer = regularizers.l2(0.0004))(cnn1)\n",
    "    #max pooling                         \n",
    "    # cnn1 = tf.keras.layers.MaxPooling1D(pool_size=3, padding='valid')(cnn1)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    ## conv aize = 4\n",
    "    cnn2 = tf.keras.layers.Conv1D(256, 4, padding='same', strides=1, \n",
    "                                    activation='relu',\\\n",
    "                                    kernel_initializer='he_normal',\\\n",
    "                                    kernel_regularizer = regularizers.l2(0.0004))(embed)\n",
    "\n",
    "    cnn2 = tf.keras.layers.Conv1D(256, 4, padding='same', strides=1, \n",
    "                                activation='relu',\\\n",
    "                                kernel_initializer='he_normal',\\\n",
    "                                kernel_regularizer = regularizers.l2(0.0004))(cnn2)\n",
    "    #max pooling                         \n",
    "    # cnn2 = tf.keras.layers.MaxPooling1D(pool_size=3, padding='valid')(cnn2)\n",
    "\n",
    "    cnn2 = tf.keras.layers.Conv1D(128, 4, padding='same', strides=1, \n",
    "                                    activation='relu',\\\n",
    "                                    kernel_initializer='he_normal',\\\n",
    "                                    kernel_regularizer = regularizers.l2(0.0004))(cnn2)\n",
    "\n",
    "    cnn2 = tf.keras.layers.Conv1D(128, 4, padding='same', strides=1, \n",
    "                                activation='relu',\\\n",
    "                                kernel_initializer='he_normal',\\\n",
    "                                kernel_regularizer = regularizers.l2(0.0004))(cnn2)\n",
    "    #max pooling                         \n",
    "    # cnn2 = tf.keras.layers.MaxPooling1D(pool_size=3, padding='valid')(cnn2)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "\n",
    "    ## conv aize = 5\n",
    "    cnn3 = tf.keras.layers.Conv1D(256, 5, padding='same', strides=1, \n",
    "                                    activation='relu',\\\n",
    "                                    kernel_initializer='he_normal',\\\n",
    "                                    kernel_regularizer = regularizers.l2(0.0004))(embed)\n",
    "\n",
    "    cnn3 = tf.keras.layers.Conv1D(256, 5, padding='same', strides=1, \n",
    "                                activation='relu',\\\n",
    "                                kernel_initializer='he_normal',\\\n",
    "                                kernel_regularizer = regularizers.l2(0.0004))(cnn3)\n",
    "    #max pooling                         \n",
    "    # cnn3 = tf.keras.layers.MaxPooling1D(pool_size=3, padding='valid')(cnn3)\n",
    "\n",
    "    cnn3 = tf.keras.layers.Conv1D(128, 5, padding='same', strides=1, \n",
    "                                    activation='relu',\\\n",
    "                                    kernel_initializer='he_normal',\\\n",
    "                                    kernel_regularizer = regularizers.l2(0.0004))(cnn3)\n",
    "\n",
    "    cnn3 = tf.keras.layers.Conv1D(128, 5, padding='same', strides=1, \n",
    "                                activation='relu',\\\n",
    "                                kernel_initializer='he_normal',\\\n",
    "                                kernel_regularizer = regularizers.l2(0.0004))(cnn3)\n",
    "    #max pooling                         \n",
    "    # cnn3 = tf.keras.layers.MaxPooling1D(pool_size=3, padding='valid')(cnn3)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "\n",
    "    # 合併三個模型的輸出向量\n",
    "    cnn = tf.keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = tf.keras.layers.Flatten()(cnn)\n",
    "\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(flat)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal')(flat) # ,\n",
    "                                # kernel_regularizer=regularizers.L1(0.1))(flat)\n",
    "    \n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal')(dense) # ,\n",
    "                                # kernel_regularizer=regularizers.L1(0.1))(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal')(dense) # ,\n",
    "                                # kernel_regularizer=regularizers.L1(0.05))(dense)\n",
    "\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal')(dense) # ,\n",
    "                                # kernel_regularizer=regularizers.L1(0.05))(dense)\n",
    "\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal')(dense) # ,\n",
    "                                # kernel_regularizer=regularizers.L1(0.05))(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal')(dense)\n",
    "\n",
    "    cnn_output = tf.keras.Model(inputs=cnn_input, outputs= dense)\n",
    "\n",
    "\n",
    "    # 構建ANN模型\n",
    "    ann_input = tf.keras.layers.Input(shape=(X_train_ann.shape[1],), dtype='float64')\n",
    "\n",
    "    dense = tf.keras.layers.Dense(128, activation='linear', kernel_initializer='he_normal',\n",
    "                                    # kernel_regularizer=regularizers.L1(0.1),\n",
    "                                    activity_regularizer=regularizers.L2(0.0004))(ann_input)\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    # bn = tf.keras.layers.BatchNormalization()(drop1)\n",
    "    dense = tf.keras.layers.Dense(128, activation='linear', kernel_initializer='he_normal',\n",
    "                                    # kernel_regularizer=regularizers.L1(0.01),\n",
    "                                    activity_regularizer=regularizers.L2(0.0004))(dense)\n",
    "    drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(64, activation='linear', kernel_initializer='he_normal',\n",
    "                                # kernel_regularizer=regularizers.L1(0.01),\n",
    "                                activity_regularizer=regularizers.L2(0.0004),)(drop1)\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(64, activation='linear', kernel_initializer='he_normal',\n",
    "                                # kernel_regularizer=regularizers.L1(0.01),\n",
    "                                activity_regularizer=regularizers.L2(0.0004),)(dense)\n",
    "    # drop1 = tf.keras.layers.Dropout(0.8)(dense)\n",
    "\n",
    "    # bn = tf.keras.layers.BatchNormalization()(drop1)\n",
    "    # dense = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal',\n",
    "    #                             kernel_regularizer=regularizers.L1(0.01),\n",
    "    #                             activity_regularizer=regularizers.L2(0.01),)(bn)\n",
    "    # dense = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal')(dense)\n",
    "\n",
    "    ann_output = tf.keras.Model(inputs=ann_input, outputs=dense)\n",
    "\n",
    "    # combine\n",
    "    combined = layers.concatenate([cnn_output.output, ann_output.output])\n",
    "\n",
    "    # bn = tf.keras.layers.BatchNormalization()(combined)\n",
    "    # dense = tf.keras.layers.Dense(128, activation=\"relu\",kernel_regularizer=regularizers.L1(0.05),)(bn)\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "    # dense = tf.keras.layers.Dense(128, activation=\"relu\")(drop1)\n",
    "    # drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "    dense = tf.keras.layers.Dense(64, activation=\"relu\")(combined) # ,kernel_regularizer=regularizers.L1(0.05),)(bn)\n",
    "    drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "    dense = tf.keras.layers.Dense(64, activation=\"relu\")(drop1) #,kernel_regularizer=regularizers.L1(0.05),)(drop1)\n",
    "    drop1 = tf.keras.layers.Dropout(0.5)(dense)\n",
    "    # dense = tf.keras.layers.Dense(64, activation=\"relu\")(drop1)\n",
    "    # drop1 = tf.keras.layers.Dropout(0.2)(dense)\n",
    "    dense = tf.keras.layers.Dense(32, activation=\"relu\")(drop1) #,kernel_regularizer=regularizers.L1(0.05),)(drop1)\n",
    "    dense = tf.keras.layers.Dense(1, activation=\"relu\")(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[cnn_output.input, ann_output.input], outputs=dense)\n",
    "\n",
    "\n",
    "    # print(model.summary())\n",
    "\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate=0.03), \n",
    "                            metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    print(model.summary())\n",
    "    \n",
    "\n",
    "    # batch_size, epochs\n",
    "    batch_size = 32 \n",
    "    epochs = 20\n",
    "    history = model.fit(x = ([X_train_cnn, X_train_ann]), \n",
    "                        y = y_train, \n",
    "                        validation_data=([X_valid_cnn, X_valid_ann], y_valid), \n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        verbose = 1)\n",
    "\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc= 'lower right')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 99)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          12800       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 128)         512         ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          16512       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           4160        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           4160        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 32)           2080        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 65)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            33          ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48,513\n",
      "Trainable params: 48,257\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "678/678 [==============================] - 4s 3ms/step - loss: 3710605568.0000 - mean_squared_error: 3710605568.0000 - val_loss: 3667883264.0000 - val_mean_squared_error: 3667883264.0000\n",
      "Epoch 2/20\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 3367487744.0000 - mean_squared_error: 3367486976.0000 - val_loss: 2234507008.0000 - val_mean_squared_error: 2234504960.0000\n",
      "Epoch 3/20\n",
      "616/678 [==========================>...] - ETA: 0s - loss: 2170300672.0000 - mean_squared_error: 2170299392.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_(X_train_cnn, X_valid_cnn, X_train_ann, X_valid_ann, y_train, y_valid, embedding_matrix)\n",
      "Cell \u001b[1;32mIn[23], line 181\u001b[0m, in \u001b[0;36mmodel_\u001b[1;34m(X_train_cnn, X_valid_cnn, X_train_ann, X_valid_ann, y_train, y_valid, embedding_matrix)\u001b[0m\n\u001b[0;32m    179\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m \n\u001b[0;32m    180\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m--> 181\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x \u001b[39m=\u001b[39;49m ([X_train_cnn, X_train_ann]), \n\u001b[0;32m    182\u001b[0m                     y \u001b[39m=\u001b[39;49m y_train, \n\u001b[0;32m    183\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m([X_valid_cnn, X_valid_ann], y_valid), \n\u001b[0;32m    184\u001b[0m                     batch_size \u001b[39m=\u001b[39;49m batch_size,\n\u001b[0;32m    185\u001b[0m                     epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[0;32m    186\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    189\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    190\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\yuhex\\anaconda3\\envs\\python_3915\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_(X_train_cnn, X_valid_cnn, X_train_ann, X_valid_ann, y_train, y_valid, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a18d44d90538e121ccc08f26e1180552e64586707c563e3afe8b38a57d2cb696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
